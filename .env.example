# Example .env file - copy to .env and fill in your values

# LLM Provider API Keys (at least one required)
OPENAI_API_KEY=sk-your-openai-key-here
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
GEMINI_API_KEY=your-gemini-key-here

# Ollama Configuration (local LLM inference)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:14b-q4
OLLAMA_TIMEOUT=5.0

# Provider Priority
PREFER_LOCAL=true  # Try Ollama first, fallback to cloud providers

# Optional: Azure OpenAI (if using Azure)
AZURE_OPENAI_API_KEY=your-azure-key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_MODEL_DEPLOYMENT=gpt-4o

# Optional: DeepSeek
DEEPSEEK_API_KEY=your-deepseek-key

# Optional: Custom provider keys
CUSTOM_API_KEY=your-custom-api-key
CUSTOM_API_URL=https://api.custom-provider.com

# Application Settings
LOG_LEVEL=INFO
ENVIRONMENT=development

# Rate Limiting
MAX_REQUESTS_PER_SECOND=2
RATE_LIMIT_ENABLED=true

# Data Storage
DATA_DIR=data
LOGS_DIR=logs
